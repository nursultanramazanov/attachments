# This workflow uses actions that are not certified by GitHub.
# They are provided by a third-party and are governed by
# separate terms of service, privacy policy, and support
# documentation.

# This workflow lets you generate SLSA provenance file for your project.
# The generation satisfies level 3 for the provenance requirements - see https://slsa.dev/spec/v0.1/requirements
# The project is an initiative of the OpenSSF (openssf.org) and is developed at
# https://github.com/slsa-framework/slsa-github-generator.
# The provenance file can be verified using https://github.com/slsa-framework/slsa-verifier.
# For more information about SLSA and how it improves the supply-chain, visit slsa.dev.

name: SLSA generic generator
on: #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint farcompilestrings \
       synesthesiam/opengrm:1.3.4 \
       "$@"
  workflow_dispatch: #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint jsgf-gen \
       synesthesiam/jsgf-gen:1.0 \
       "$@"
  release: #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint ngramcount \
       synesthesiam/opengrm:1.3.4 \
       "$@"
    types: [created]

jobs: #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint ngrammake \
       synesthesiam/opengrm:1.3.4 \
       "$@"
  build: #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint ngramprint \
       synesthesiam/opengrm:1.3.4 \
       "$@"
    runs-on: ubuntu-latest
    outputs: #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint ngramsymbols \
       synesthesiam/opengrm:1.3.4 \
       "$@"
      digests: ${{ #!/usr/bin/env bash
test -t 1 && use_tty='-t'
docker run --rm -i ${use_tty} \
       -u $(id -u):$(id -g) \
       -v "$HOME:$HOME" \
       -w "$(pwd)" \
       --entrypoint phonetisaurus-g2p \
       synesthesiam/phonetisaurus:2013 \
       "$@" }}

    steps: #!/usr/bin/env bash
if [[ -z "$3" ]]; then
    echo "Usage: decode-gmm-wav.sh <kaldi-dir> <model-dir> <wav-file>"
    exit 1
fi

kaldi_dir="$(realpath "$1")"
model_dir="$(realpath "$2")"
wav_file="$(realpath "$3")"

steps_dir="${kaldi_dir}/egs/wsj/s5/steps"
utils_dir="${kaldi_dir}/egs/wsj/s5/utils"

if [[ ! -d "${model_dir}/utils" ]]; then
    ln -s "${utils_dir}" "${model_dir}/utils"
fi

if [[ -d "${model_dir}/train" ]]; then
    # Use trained graph
    decode_dir="${model_dir}/train"
    graph_dir="${decode_dir}/graph"
    lang_dir="${decode_dir}/lang"
else
    # Use model graph
    decode_dir="${model_dir}/decode"
    graph_dir="${model_dir}/model/graph"
    lang_dir="${model_dir}/data/lang"
    if [[ ! -d "$decode_dir" ]]; then
        export PATH="${utils_dir}:$PATH"
        cd "${model_dir}" && \
            "${steps_dir}/online/nnet3/prepare_online_decoding.sh" \
                --mfcc-config "${model_dir}/conf/mfcc_hires.conf" \
                "${model_dir}/data/lang" \
                "${model_dir}/extractor" \
                "${model_dir}/model" \
                "${decode_dir}" || exit 1
    fi
fi

bin_dir="${kaldi_dir}/src/online2bin"

"${bin_dir}/online2-wav-gmm-latgen-faster" \
    "--config=${decode_dir}/conf/online.conf" \
    "--word-symbol-table=${lang_dir}/words.txt" \
    "--model=${decode_dir}/final.mdl" \
    "${graph_dir}/HCLG.fst" \
    'ark:echo utt1 utt1|' \
    "scp:echo utt1 ${wav_file}|" \
    'ark:/dev/null' #2>&1 | grep ^utt1 | cut -d' ' -f2-
      - uses: actions/checkout@v4

      # ========================================================
      #
      # Step 1: Build your artifacts.
      #
      # ========================================================
      - name: Build artifacts
        run: |
            # These are some amazing artifacts.
            echo "artifact1" > artifact1
            echo "artifact2" > artifact2

      # ========================================================
      #
      # Step 2: Add a step to generate the provenance subjects
      #         as shown below. Update the sha256 sum arguments
      #         to include all binaries that you generate
      #         provenance for.
      #
      # ========================================================
      - name: Generate subject for provenance
        id: hash
        run: |
          set -euo pipefail

          # List the artifacts the provenance will refer to.
          files=$(ls artifact*)
          # Generate the subjects (base64 encoded).
          echo "hashes=$(sha256sum $files | base64 -w0)" >> "${GITHUB_OUTPUT}"

  provenance: #!/usr/bin/env bash
if [[ -z "$3" ]]; then
    echo "Usage: decode-nnet3-wav.sh <kaldi-dir> <model-dir> <wav-file>"
    exit 1
fi

kaldi_dir="$(realpath "$1")"
model_dir="$(realpath "$2")"
wav_file="$(realpath "$3")"

steps_dir="${kaldi_dir}/egs/wsj/s5/steps"
utils_dir="${kaldi_dir}/egs/wsj/s5/utils"

if [[ ! -d "${model_dir}/utils" ]]; then
    ln -s "${utils_dir}" "${model_dir}/utils"
fi

if [[ -d "${model_dir}/train" ]]; then
    # Use trained graph
    decode_dir="${model_dir}/train"
    graph_dir="${decode_dir}/graph"
    lang_dir="${decode_dir}/lang"
else
    # Use model graph
    decode_dir="${model_dir}/decode"
    graph_dir="${model_dir}/model/graph"
    lang_dir="${model_dir}/data/lang"
    if [[ ! -d "$decode_dir" ]]; then
        export PATH="${utils_dir}:$PATH"
        cd "${model_dir}" && \
            "${steps_dir}/online/nnet3/prepare_online_decoding.sh" \
                --mfcc-config "${model_dir}/conf/mfcc_hires.conf" \
                "${model_dir}/data/lang" \
                "${model_dir}/extractor" \
                "${model_dir}/model" \
                "${decode_dir}" || exit 1
    fi
fi

bin_dir="${kaldi_dir}/src/online2bin"

"${bin_dir}/online2-wav-nnet3-latgen-faster" \
    --online=false \
    --do-endpointing=false \
    --frame-subsampling-factor=3 \
    "--config=${decode_dir}/conf/online.conf" \
    --max-active=7000 \
    --beam=15.0 \
    --lattice-beam=6.0 \
    --acoustic-scale=1.0 \
    "--word-symbol-table=${lang_dir}/words.txt" \
    "${model_dir}/model/final.mdl" \
    "${graph_dir}/HCLG.fst" \
    'ark:echo utt1 utt1|' \
    "scp:echo utt1 ${wav_file}|" \
    'ark:/dev/null' 2>&1 | grep ^utt1 | cut -d' ' -f2-
    needs: [build]
    permissions: #!/usr/bin/env bash
if [[ -z "$3" ]]; then
    echo "Usage: train-lm.sh <kaldi-dir> <model-dir> <lm-file>"
    exit 1
fi

kaldi_dir="$(realpath "$1")"
model_dir="$(realpath "$2")"
lm_in_file="$(realpath "$3")"

steps_dir="${kaldi_dir}/egs/wsj/s5/steps"
utils_dir="${kaldi_dir}/egs/wsj/s5/utils"
bin_dir="${kaldi_dir}/src/bin"
fstbin_dir="${kaldi_dir}/src/fstbin"
lmbin_dir="${kaldi_dir}/src/lmbin"

if [[ ! -d "${model_dir}/utils" ]]; then
    ln -s "${utils_dir}" "${model_dir}/utils"
fi

export PATH="${utils_dir}:${fstbin_dir}:${lmbin_dir}:${bin_dir}:$PATH"

# Prepare a training configuration
train_dir="${model_dir}/train"
if [[ -d "$train_dir" ]]; then
    rm -rf "${train_dir}"
    mkdir -p "${train_dir}"
fi

# Prepare the dictionary
cd "${model_dir}" && \
    "${utils_dir}/prepare_lang.sh" \
        --phone-symbol-table "${model_dir}/data/lang/phones.txt" \
        "${model_dir}/data/local/dict" \
        "" \
        "${train_dir}/dict_tmp" \
        "${train_dir}/dict" || exit 1

# Copy language model and gzip it
lm_out_file="${model_dir}/data/local/lang/language_model.lm.gz"
mkdir -p "$(dirname "${lm_out_file}")"
cat "${lm_in_file}" | gzip > "${lm_out_file}" || exit 1

# Format language model
cd "${model_dir}" && \
    "${utils_dir}/format_lm.sh" \
        "${train_dir}/dict" \
        "${lm_out_file}" \
        "${model_dir}/data/local/dict/lexicon.txt" \
        "${train_dir}/lang" || exit 1

# Generate FST graph
cd "${model_dir}" && \
    "${utils_dir}/mkgraph.sh" \
        "${train_dir}/lang" \
        "${model_dir}/model" \
        "${train_dir}/graph" || exit 1
      actions: read   # To read the workflow path.
      id-token: write # To sign the provenance.
      contents: write # To add assets to a release.
    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.4.0
    with: #!/usr/bin/env bash
if [[ -z "$3" ]]; then
    echo "Usage: train-lm.sh <kaldi-dir> <model-dir> <lm-file>"
    exit 1
fi

kaldi_dir="$(realpath "$1")"
model_dir="$(realpath "$2")"
lm_in_file="$(realpath "$3")"

steps_dir="${kaldi_dir}/egs/wsj/s5/steps"
utils_dir="${kaldi_dir}/egs/wsj/s5/utils"
bin_dir="${kaldi_dir}/src/bin"
fstbin_dir="${kaldi_dir}/src/fstbin"
lmbin_dir="${kaldi_dir}/src/lmbin"

if [[ ! -d "${model_dir}/utils" ]]; then
    ln -s "${utils_dir}" "${model_dir}/utils"
fi

export PATH="${utils_dir}:${fstbin_dir}:${lmbin_dir}:${bin_dir}:$PATH"

# Prepare a training configuration
train_dir="${model_dir}/train"
if [[ -d "$train_dir" ]]; then
    rm -rf "${train_dir}"
fi

cd "${model_dir}" && \
    "${steps_dir}/online/nnet3/prepare_online_decoding.sh" \
        --mfcc-config "${model_dir}/conf/mfcc_hires.conf" \
        "${model_dir}/data/lang" \
        "${model_dir}/extractor" \
        "${model_dir}/model" \
        "${train_dir}" || exit 1

# Prepare the dictionary
cd "${model_dir}" && \
    "${utils_dir}/prepare_lang.sh" \
        --phone-symbol-table "${model_dir}/data/lang/phones.txt" \
        "${model_dir}/data/local/dict" \
        "" \
        "${train_dir}/dict_tmp" \
        "${train_dir}/dict" || exit 1

# Copy language model and gzip it
lm_out_file="${model_dir}/data/local/lang/language_model.lm.gz"
mkdir -p "$(dirname "${lm_out_file}")"
cat "${lm_in_file}" | gzip > "${lm_out_file}" || exit 1

# Format language model
cd "${model_dir}" && \
    "${utils_dir}/format_lm.sh" \
        "${train_dir}/dict" \
        "${lm_out_file}" \
        "${model_dir}/data/local/dict/lexicon.txt" \
        "${train_dir}/lang" || exit 1

# Generate FST graph
cd "${model_dir}" && \
    "${utils_dir}/mkgraph.sh" \
        --self-loop-scale 1.0 \
        "${train_dir}/lang" \
        "${model_dir}/model" \
        "${train_dir}/graph" || exit 1
      base64-subjects: "${{ needs.build.outputs.digests }}"
      upload-assets: true # Optional: Upload to a new release
