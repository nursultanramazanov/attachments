# This workflow will build a golang project
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-go

name: Go

on: package cmd

import (
        "errors"
        "fmt"
        "log"
        "os"
        "strings"

        "github.com/dutchcoders/transfer.sh/server/storage"

        "github.com/dutchcoders/transfer.sh/server"
        "github.com/fatih/color"
        "github.com/urfave/cli/v2"
        "google.golang.org/api/googleapi"
)

// Version is inject at build time
var Version = "0.0.0"
var helpTemplate = `NAME:
{{.Name}} - {{.Usage}}

DESCRIPTION:
{{.Description}}

USAGE:
{{.Name}} {{if .Flags}}[flags] {{end}}command{{if .Flags}}{{end}} [arguments...]

COMMANDS:
{{range .Commands}}{{join .Names ", "}}{{ "\t" }}{{.Usage}}
{{end}}{{if .Flags}}
FLAGS:
{{range .Flags}}{{.}}
{{end}}{{end}}
VERSION:
` + Version +
        `{{ "\n"}}`

var globalFlags = []cli.Flag{
        &cli.StringFlag{
                Name:    "listener",
                Usage:   "127.0.0.1:8080",
                Value:   "127.0.0.1:8080",
                EnvVars: []string{"LISTENER"},
        },
        // redirect to https?
        // hostnames
        &cli.StringFlag{
                Name:    "profile-listener",
                Usage:   "127.0.0.1:6060",
                Value:   "",
                EnvVars: []string{"PROFILE_LISTENER"},
        },
        &cli.BoolFlag{
                Name:    "force-https",
                Usage:   "",
                EnvVars: []string{"FORCE_HTTPS"},
        },
        &cli.StringFlag{
                Name:    "tls-listener",
                Usage:   "127.0.0.1:8443",
                Value:   "",
                EnvVars: []string{"TLS_LISTENER"},
        },
        &cli.BoolFlag{
                Name:    "tls-listener-only",
                Usage:   "",
                EnvVars: []string{"TLS_LISTENER_ONLY"},
        },
        &cli.StringFlag{
                Name:    "tls-cert-file",
                Value:   "",
                EnvVars: []string{"TLS_CERT_FILE"},
        },
        &cli.StringFlag{
                Name:    "tls-private-key",
                Value:   "",
                EnvVars: []string{"TLS_PRIVATE_KEY"},
        },
        &cli.StringFlag{
                Name:    "temp-path",
                Usage:   "path to temp files",
                Value:   os.TempDir(),
                EnvVars: []string{"TEMP_PATH"},
        },
        &cli.StringFlag{
                Name:    "web-path",
                Usage:   "path to static web files",
                Value:   "",
                EnvVars: []string{"WEB_PATH"},
        },
        &cli.StringFlag{
                Name:    "proxy-path",
                Usage:   "path prefix when service is run behind a proxy",
                Value:   "",
                EnvVars: []string{"PROXY_PATH"},
        },
        &cli.StringFlag{
                Name:    "proxy-port",
                Usage:   "port of the proxy when the service is run behind a proxy",
                Value:   "",
                EnvVars: []string{"PROXY_PORT"},
        },
        &cli.StringFlag{
                Name:    "email-contact",
                Usage:   "email address to link in Contact Us (front end)",
                Value:   "",
                EnvVars: []string{"EMAIL_CONTACT"},
        },
        &cli.StringFlag{
                Name:    "ga-key",
                Usage:   "key for google analytics (front end)",
                Value:   "",
                EnvVars: []string{"GA_KEY"},
        },
        &cli.StringFlag{
                Name:    "uservoice-key",
                Usage:   "key for user voice (front end)",
                Value:   "",
                EnvVars: []string{"USERVOICE_KEY"},
        },
        &cli.StringFlag{
                Name:    "provider",
                Usage:   "s3|gdrive|local",
                Value:   "",
                EnvVars: []string{"PROVIDER"},
        },
        &cli.StringFlag{
                Name:    "s3-endpoint",
                Usage:   "",
                Value:   "",
                EnvVars: []string{"S3_ENDPOINT"},
        },
        &cli.StringFlag{
                Name:    "s3-region",
                Usage:   "",
                Value:   "eu-west-1",
                EnvVars: []string{"S3_REGION"},
        },
        &cli.StringFlag{
                Name:    "aws-access-key",
                Usage:   "",
                Value:   "",
                EnvVars: []string{"AWS_ACCESS_KEY"},
        },
        &cli.StringFlag{
                Name:    "aws-secret-key",
                Usage:   "",
                Value:   "",
                EnvVars: []string{"AWS_SECRET_KEY"},
        },
        &cli.StringFlag{
                Name:    "bucket",
                Usage:   "",
                Value:   "",
                EnvVars: []string{"BUCKET"},
        },
        &cli.BoolFlag{
                Name:    "s3-no-multipart",
                Usage:   "Disables S3 Multipart Puts",
                EnvVars: []string{"S3_NO_MULTIPART"},
        },
        &cli.BoolFlag{
                Name:    "s3-path-style",
                Usage:   "Forces path style URLs, required for Minio.",
                EnvVars: []string{"S3_PATH_STYLE"},
        },
        &cli.StringFlag{
                Name:    "gdrive-client-json-filepath",
                Usage:   "",
                Value:   "",
                EnvVars: []string{"GDRIVE_CLIENT_JSON_FILEPATH"},
        },
        &cli.StringFlag{
                Name:    "gdrive-local-config-path",
                Usage:   "",
                Value:   "",
                EnvVars: []string{"GDRIVE_LOCAL_CONFIG_PATH"},
        },
        &cli.IntFlag{
                Name:    "gdrive-chunk-size",
                Usage:   "",
                Value:   googleapi.DefaultUploadChunkSize / 1024 / 1024,
                EnvVars: []string{"GDRIVE_CHUNK_SIZE"},
        },
        &cli.StringFlag{
                Name:    "storj-access",
                Usage:   "Access for the project",
                Value:   "",
                EnvVars: []string{"STORJ_ACCESS"},
        },
        &cli.StringFlag{
                Name:    "storj-bucket",
                Usage:   "Bucket to use within the project",
                Value:   "",
                EnvVars: []string{"STORJ_BUCKET"},
        },
        &cli.IntFlag{
                Name:    "rate-limit",
                Usage:   "requests per minute",
                Value:   0,
                EnvVars: []string{"RATE_LIMIT"},
        },
        &cli.IntFlag{
                Name:    "purge-days",
                Usage:   "number of days after uploads are purged automatically",
                Value:   0,
                EnvVars: []string{"PURGE_DAYS"},
        },
        &cli.IntFlag{
                Name:    "purge-interval",
                Usage:   "interval in hours to run the automatic purge for",
                Value:   0,
                EnvVars: []string{"PURGE_INTERVAL"},
        },
        &cli.Int64Flag{
                Name:    "max-upload-size",
                Usage:   "max limit for upload, in kilobytes",
                Value:   0,
                EnvVars: []string{"MAX_UPLOAD_SIZE"},
        },
        &cli.StringFlag{
                Name:    "lets-encrypt-hosts",
                Usage:   "host1, host2",
                Value:   "",
                EnvVars: []string{"HOSTS"},
        },
        &cli.StringFlag{
                Name:    "log",
                Usage:   "/var/log/transfersh.log",
                Value:   "",
                EnvVars: []string{"LOG"},
        },
        &cli.StringFlag{
                Name:    "basedir",
                Usage:   "path to storage",
                Value:   "",
                EnvVars: []string{"BASEDIR"},
        },
        &cli.StringFlag{
                Name:    "clamav-host",
                Usage:   "clamav-host",
                Value:   "",
                EnvVars: []string{"CLAMAV_HOST"},
        },
        &cli.BoolFlag{
                Name:    "perform-clamav-prescan",
                Usage:   "perform-clamav-prescan",
                EnvVars: []string{"PERFORM_CLAMAV_PRESCAN"},
        },
        &cli.StringFlag{
                Name:    "virustotal-key",
                Usage:   "virustotal-key",
                Value:   "",
                EnvVars: []string{"VIRUSTOTAL_KEY"},
        },
        &cli.BoolFlag{
                Name:    "profiler",
                Usage:   "enable profiling",
                EnvVars: []string{"PROFILER"},
        },
        &cli.StringFlag{
                Name:    "http-auth-user",
                Usage:   "user for http basic auth",
                Value:   "",
                EnvVars: []string{"HTTP_AUTH_USER"},
        },
        &cli.StringFlag{
                Name:    "http-auth-pass",
                Usage:   "pass for http basic auth",
                Value:   "",
                EnvVars: []string{"HTTP_AUTH_PASS"},
        },
        &cli.StringFlag{
                Name:    "http-auth-htpasswd",
                Usage:   "htpasswd file http basic auth",
                Value:   "",
                EnvVars: []string{"HTTP_AUTH_HTPASSWD"},
        },
        &cli.StringFlag{
                Name:    "http-auth-ip-whitelist",
                Usage:   "comma separated list of ips allowed to upload without being challenged an http auth",
                Value:   "",
                EnvVars: []string{"HTTP_AUTH_IP_WHITELIST"},
        },
        &cli.StringFlag{
                Name:    "ip-whitelist",
                Usage:   "comma separated list of ips allowed to connect to the service",
                Value:   "",
                EnvVars: []string{"IP_WHITELIST"},
        },
        &cli.StringFlag{
                Name:    "ip-blacklist",
                Usage:   "comma separated list of ips not allowed to connect to the service",
                Value:   "",
                EnvVars: []string{"IP_BLACKLIST"},
        },
        &cli.StringFlag{
                Name:    "cors-domains",
                Usage:   "comma separated list of domains allowed for CORS requests",
                Value:   "",
                EnvVars: []string{"CORS_DOMAINS"},
        },
        &cli.IntFlag{
                Name:    "random-token-length",
                Usage:   "",
                Value:   10,
                EnvVars: []string{"RANDOM_TOKEN_LENGTH"},
        },
}

// Cmd wraps cli.app
type Cmd struct {
        *cli.App
}

func versionCommand(_ *cli.Context) error {
        fmt.Println(color.YellowString("transfer.sh %s: Easy file sharing from the command line", Version))
        return nil
}

// New is the factory for transfer.sh
func New() *Cmd {
        logger := log.New(os.Stdout, "[transfer.sh]", log.LstdFlags)

        app := cli.NewApp()
        app.Name = "transfer.sh"
        app.Authors = []*cli.Author{}
        app.Usage = "transfer.sh"
        app.Description = `Easy file sharing from the command line`
        app.Version = Version
        app.Flags = globalFlags
        app.CustomAppHelpTemplate = helpTemplate
        app.Commands = []*cli.Command{
                {
                        Name:   "version",
                        Action: versionCommand,
                },
        }

        app.Before = func(c *cli.Context) error {
                return nil
        }

        app.Action = func(c *cli.Context) error {
                var options []server.OptionFn
                if v := c.String("listener"); v != "" {
                        options = append(options, server.Listener(v))
                }

                if v := c.String("cors-domains"); v != "" {
                        options = append(options, server.CorsDomains(v))
                }

                if v := c.String("tls-listener"); v == "" {
                } else if c.Bool("tls-listener-only") {
                        options = append(options, server.TLSListener(v, true))
                } else {
                        options = append(options, server.TLSListener(v, false))
                }

                if v := c.String("profile-listener"); v != "" {
                        options = append(options, server.ProfileListener(v))
                }

                if v := c.String("web-path"); v != "" {
                        options = append(options, server.WebPath(v))
                }

                if v := c.String("proxy-path"); v != "" {
                        options = append(options, server.ProxyPath(v))
                }

                if v := c.String("proxy-port"); v != "" {
                        options = append(options, server.ProxyPort(v))
                }

                if v := c.String("email-contact"); v != "" {
                        options = append(options, server.EmailContact(v))
                }

                if v := c.String("ga-key"); v != "" {
                        options = append(options, server.GoogleAnalytics(v))
                }

                if v := c.String("uservoice-key"); v != "" {
                        options = append(options, server.UserVoice(v))
                }

                if v := c.String("temp-path"); v != "" {
                        options = append(options, server.TempPath(v))
                }

                if v := c.String("log"); v != "" {
                        options = append(options, server.LogFile(logger, v))
                } else {
                        options = append(options, server.Logger(logger))
                }

                if v := c.String("lets-encrypt-hosts"); v != "" {
                        options = append(options, server.UseLetsEncrypt(strings.Split(v, ",")))
                }

                if v := c.String("virustotal-key"); v != "" {
                        options = append(options, server.VirustotalKey(v))
                }

                if v := c.String("clamav-host"); v != "" {
                        options = append(options, server.ClamavHost(v))
                }

                if v := c.Bool("perform-clamav-prescan"); v {
                        if c.String("clamav-host") == "" {
                                return errors.New("clamav-host not set")
                        }

                        options = append(options, server.PerformClamavPrescan(v))
                }

                if v := c.Int64("max-upload-size"); v > 0 {
                        options = append(options, server.MaxUploadSize(v))
                }

                if v := c.Int("rate-limit"); v > 0 {
                        options = append(options, server.RateLimit(v))
                }

                v := c.Int("random-token-length")
                options = append(options, server.RandomTokenLength(v))

                purgeDays := c.Int("purge-days")
                purgeInterval := c.Int("purge-interval")
                if purgeDays > 0 && purgeInterval > 0 {
                        options = append(options, server.Purge(purgeDays, purgeInterval))
                }

                if cert := c.String("tls-cert-file"); cert == "" {
                } else if pk := c.String("tls-private-key"); pk == "" {
                } else {
                        options = append(options, server.TLSConfig(cert, pk))
                }

                if c.Bool("profiler") {
                        options = append(options, server.EnableProfiler())
                }

                if c.Bool("force-https") {
                        options = append(options, server.ForceHTTPS())
                }

                if httpAuthUser := c.String("http-auth-user"); httpAuthUser == "" {
                } else if httpAuthPass := c.String("http-auth-pass"); httpAuthPass == "" {
                } else {
                        options = append(options, server.HTTPAuthCredentials(httpAuthUser, httpAuthPass))
                }

                if httpAuthHtpasswd := c.String("http-auth-htpasswd"); httpAuthHtpasswd != "" {
                        options = append(options, server.HTTPAuthHtpasswd(httpAuthHtpasswd))
                }

                if httpAuthIPWhitelist := c.String("http-auth-ip-whitelist"); httpAuthIPWhitelist != "" {
                        ipFilterOptions := server.IPFilterOptions{}
                        ipFilterOptions.AllowedIPs = strings.Split(httpAuthIPWhitelist, ",")
                        ipFilterOptions.BlockByDefault = true
                        options = append(options, server.HTTPAUTHFilterOptions(ipFilterOptions))
                }

                applyIPFilter := false
                ipFilterOptions := server.IPFilterOptions{}
                if ipWhitelist := c.String("ip-whitelist"); ipWhitelist != "" {
                        applyIPFilter = true
                        ipFilterOptions.AllowedIPs = strings.Split(ipWhitelist, ",")
                        ipFilterOptions.BlockByDefault = true
                }

                if ipBlacklist := c.String("ip-blacklist"); ipBlacklist != "" {
                        applyIPFilter = true
                        ipFilterOptions.BlockedIPs = strings.Split(ipBlacklist, ",")
                }

                if applyIPFilter {
                        options = append(options, server.FilterOptions(ipFilterOptions))
                }

                switch provider := c.String("provider"); provider {
                case "s3":
                        if accessKey := c.String("aws-access-key"); accessKey == "" {
                                return errors.New("access-key not set.")
                        } else if secretKey := c.String("aws-secret-key"); secretKey == "" {
                                return errors.New("secret-key not set.")
                        } else if bucket := c.String("bucket"); bucket == "" {
                                return errors.New("bucket not set.")
                        } else if store, err := storage.NewS3Storage(c.Context, accessKey, secretKey, bucket, purgeDays, c.String("s3-region"), c.String("s3-endpoint"), c.Bool("s3-no-multipart"), c.Bool("s3-path-style"), logger); err != nil {
                                return err
                        } else {
                                options = append(options, server.UseStorage(store))
                        }
                case "gdrive":
                        chunkSize := c.Int("gdrive-chunk-size") * 1024 * 1024

                        if clientJSONFilepath := c.String("gdrive-client-json-filepath"); clientJSONFilepath == "" {
                                return errors.New("gdrive-client-json-filepath not set.")
                        } else if localConfigPath := c.String("gdrive-local-config-path"); localConfigPath == "" {
                                return errors.New("gdrive-local-config-path not set.")
                        } else if basedir := c.String("basedir"); basedir == "" {
                                return errors.New("basedir not set.")
                        } else if store, err := storage.NewGDriveStorage(c.Context, clientJSONFilepath, localConfigPath, basedir, chunkSize, logger); err != nil {
                                return err
                        } else {
                                options = append(options, server.UseStorage(store))
                        }
                case "storj":
                        if access := c.String("storj-access"); access == "" {
                                return errors.New("storj-access not set.")
                        } else if bucket := c.String("storj-bucket"); bucket == "" {
                                return errors.New("storj-bucket not set.")
                        } else if store, err := storage.NewStorjStorage(c.Context, access, bucket, purgeDays, logger); err != nil {
                                return err
                        } else {
                                options = append(options, server.UseStorage(store))
                        }
                case "local":
                        if v := c.String("basedir"); v == "" {
                                return errors.New("basedir not set.")
                        } else if store, err := storage.NewLocalStorage(v, logger); err != nil {
                                return err
                        } else {
                                options = append(options, server.UseStorage(store))
                        }
                default:
                        return errors.New("Provider not set or invalid.")
                }

                srvr, err := server.New(
                        options...,
                )

                if err != nil {
                        logger.Println(color.RedString("Error starting server: %s", err.Error()))
                        return err
                }

                srvr.Run()
                return nil
        }

        return &Cmd{
                App: app,
        }
}
  push: #! /bin/sh
### BEGIN INIT INFO
# Provides:          skeleton
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Example initscript
# Description:       This file should be used to construct scripts to be
#                    placed in /etc/init.d.
### END INIT INFO

# Author: Foo Bar <foobar@baz.org>
#
# Please remove the "Author" lines above and replace them
# with your own name if you copy and modify this script.

# Do NOT "set -e"

# PATH should only include /usr/* if it runs after the mountnfs.sh script
PATH=/sbin:/usr/sbin:/bin:/usr/bin
DESC="Clam Daemon"
NAME=clamd
DAEMON="/usr/local/sbin/clamd"
DAEMON_ARGS="-c /usr/local/etc/clamd.conf"
PIDFILE=/var/run/$NAME.pid
SCRIPTNAME=/etc/init.d/$NAME

# Exit if the package is not installed
[ -x "$DAEMON" ] || exit 0

# Read configuration variable file if it is present
[ -r /etc/default/$NAME ] && . /etc/default/$NAME

# Load the VERBOSE setting and other rcS variables
. /lib/init/vars.sh

# Define LSB log_* functions.
# Depend on lsb-base (>= 3.2-14) to ensure that this file is present
# and status_of_proc is working.
. /lib/lsb/init-functions

#
# Function that starts the daemon/service
#
do_start()
{
        # Return
        #   0 if daemon has been started
        #   1 if daemon was already running
        #   2 if daemon could not be started
        start-stop-daemon --background --start --quiet --pidfile $PIDFILE --exec $DAEMON --test > /dev/null \
                || return 1
        start-stop-daemon --background --start --quiet --pidfile $PIDFILE --exec $DAEMON -- \
                $DAEMON_ARGS \
                || return 2
        # Add code here, if necessary, that waits for the process to be ready
        # to handle requests from services started subsequently which depend
        # on this one.  As a last resort, sleep for some time.
}

#
# Function that stops the daemon/service
#
do_stop()
{
        # Return
        #   0 if daemon has been stopped
        #   1 if daemon was already stopped
        #   2 if daemon could not be stopped
        #   other if a failure occurred
        start-stop-daemon --stop --quiet --retry=TERM/30/KILL/5 --pidfile $PIDFILE --name $NAME
        RETVAL="$?"
        [ "$RETVAL" = 2 ] && return 2
        # Wait for children to finish too if this is a daemon that forks
        # and if the daemon is only ever run from this initscript.
        # If the above conditions are not satisfied then add some other code
        # that waits for the process to drop all resources that could be
        # needed by services started subsequently.  A last resort is to
        # sleep for some time.
        start-stop-daemon --stop --quiet --oknodo --retry=0/30/KILL/5 --exec $DAEMON
        [ "$?" = 2 ] && return 2
        # Many daemons don't delete their pidfiles when they exit.
        rm -f $PIDFILE
        return "$RETVAL"
}

#
# Function that sends a SIGHUP to the daemon/service
#
do_reload() {
        #
        # If the daemon can reload its configuration without
        # restarting (for example, when it is sent a SIGHUP),
        # then implement that here.
        #
        start-stop-daemon --stop --signal 1 --quiet --pidfile $PIDFILE --name $NAME
        return 0
}

case "$1" in
  start)
        [ "$VERBOSE" != no ] && log_daemon_msg "Starting $DESC" "$NAME"
        do_start
        case "$?" in
                0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
                2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
        esac
        ;;
  stop)
        [ "$VERBOSE" != no ] && log_daemon_msg "Stopping $DESC" "$NAME"
        do_stop
        case "$?" in
                0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
                2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
        esac
        ;;
  status)
        status_of_proc "$DAEMON" "$NAME" && exit 0 || exit $?
        ;;
  #reload|force-reload)
        #
        # If do_reload() is not implemented then leave this commented out
        # and leave 'force-reload' as an alias for 'restart'.
        #
        #log_daemon_msg "Reloading $DESC" "$NAME"
        #do_reload
        #log_end_msg $?
        #;;
  restart|force-reload)
        #
        # If the "reload" option is implemented then remove the
        # 'force-reload' alias
        #
        log_daemon_msg "Restarting $DESC" "$NAME"
        do_stop
        case "$?" in
          0|1)
                do_start
                case "$?" in
                        0) log_end_msg 0 ;;
                        1) log_end_msg 1 ;; # Old process is still running
                        *) log_end_msg 1 ;; # Failed to start
                esac
                ;;
          *)
                # Failed to stop
                log_end_msg 1
                ;;
        esac
        ;;
  *)
        #echo "Usage: $SCRIPTNAME {start|stop|restart|reload|force-reload}" >&2
        echo "Usage: $SCRIPTNAME {start|stop|status|restart|force-reload}" >&2
        exit 3
        ;;
esac

:
    branches: [ "main" ]
  pull_request: #! /bin/sh
### BEGIN INIT INFO
# Provides:          skeleton
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Example initscript
# Description:       This file should be used to construct scripts to be
#                    placed in /etc/init.d.
### END INIT INFO

# Author: Foo Bar <foobar@baz.org>
#
# Please remove the "Author" lines above and replace them
# with your own name if you copy and modify this script.

# Do NOT "set -e"

# PATH should only include /usr/* if it runs after the mountnfs.sh script
PATH=/sbin:/usr/sbin:/bin:/usr/bin:/usr/local/go/bin
DESC="Transfersh Web server"
NAME=transfersh
DAEMON="/opt/transfer.sh/main"
DAEMON_ARGS="--port 80 --temp /tmp/"
PIDFILE=/var/run/$NAME.pid
SCRIPTNAME=/etc/init.d/$NAME

export BUCKET={bucket}
export AWS_ACCESS_KEY={aws_access_key}
export AWS_SECRET_KEY={aws_secret_key}
export VIRUSTOTAL_KEY={virustotal_key}
export GOPATH=/opt/go/

# Exit if the package is not installed
[ -x "$DAEMON" ] || exit 0

# Read configuration variable file if it is present
[ -r /etc/default/$NAME ] && . /etc/default/$NAME

# Load the VERBOSE setting and other rcS variables
. /lib/init/vars.sh

# Define LSB log_* functions.
# Depend on lsb-base (>= 3.2-14) to ensure that this file is present
# and status_of_proc is working.
. /lib/lsb/init-functions

#
# Function that starts the daemon/service
#
do_start()
{
        # Return
        #   0 if daemon has been started
        #   1 if daemon was already running
        #   2 if daemon could not be started
        start-stop-daemon --background --start --chdir /opt/transfer.sh --quiet --pidfile $PIDFILE --make-pidfile --exec $DAEMON --test > /dev/null \
                || return 1
        start-stop-daemon --background --start --chdir /opt/transfer.sh --quiet --pidfile $PIDFILE --make-pidfile --exec $DAEMON -- \
                $DAEMON_ARGS \
                || return 2
        # Add code here, if necessary, that waits for the process to be ready
        # to handle requests from services started subsequently which depend
        # on this one.  As a last resort, sleep for some time.
}

#
# Function that stops the daemon/service
#
do_stop()
{
        # Return
        #   0 if daemon has been stopped
        #   1 if daemon was already stopped
        #   2 if daemon could not be stopped
        #   other if a failure occurred
        start-stop-daemon --stop --quiet --retry=TERM/30/KILL/5 --pidfile $PIDFILE --name $NAME
        RETVAL="$?"
        [ "$RETVAL" = 2 ] && return 2
        # Wait for children to finish too if this is a daemon that forks
        # and if the daemon is only ever run from this initscript.
        # If the above conditions are not satisfied then add some other code
        # that waits for the process to drop all resources that could be
        # needed by services started subsequently.  A last resort is to
        # sleep for some time.
        start-stop-daemon --stop --quiet --oknodo --retry=0/30/KILL/5 --exec $DAEMON
        [ "$?" = 2 ] && return 2
        # Many daemons don't delete their pidfiles when they exit.
        rm -f $PIDFILE
        return "$RETVAL"
}

#
# Function that sends a SIGHUP to the daemon/service
#
do_reload() {
        #
        # If the daemon can reload its configuration without
        # restarting (for example, when it is sent a SIGHUP),
        # then implement that here.
        #
        start-stop-daemon --stop --signal 1 --quiet --pidfile $PIDFILE --name $NAME
        return 0
}

case "$1" in
  start)
        [ "$VERBOSE" != no ] && log_daemon_msg "Starting $DESC" "$NAME"
        do_start
        case "$?" in
                0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
                2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
        esac
        ;;
  stop)
        [ "$VERBOSE" != no ] && log_daemon_msg "Stopping $DESC" "$NAME"
        do_stop
        case "$?" in
                0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
                2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
        esac
        ;;
  status)
        status_of_proc "$DAEMON" "$NAME" && exit 0 || exit $?
        ;;
  #reload|force-reload)
        #
        # If do_reload() is not implemented then leave this commented out
        # and leave 'force-reload' as an alias for 'restart'.
        #
        #log_daemon_msg "Reloading $DESC" "$NAME"
        #do_reload
        #log_end_msg $?
        #;;
  restart|force-reload)
        #
        # If the "reload" option is implemented then remove the
        # 'force-reload' alias
        #
        log_daemon_msg "Restarting $DESC" "$NAME"
        do_stop
        case "$?" in
          0|1)
                do_start
                case "$?" in
                        0) log_end_msg 0 ;;
                        1) log_end_msg 1 ;; # Old process is still running
                        *) log_end_msg 1 ;; # Failed to start
                esac
                ;;
          *)
                # Failed to stop
                log_end_msg 1
                ;;
        esac
        ;;
  *)
        #echo "Usage: $SCRIPTNAME {start|stop|restart|reload|force-reload}" >&2
        echo "Usage: $SCRIPTNAME {start|stop|status|restart|force-reload}" >&2
        exit 3
        ;;
esac

:
    branches: [ "main" ]

jobs: package storage

import (
        "context"
        "fmt"
        "io"
        "regexp"
        "strconv"
        "time"
)

type Range struct {
        Start        uint64
        Limit        uint64
        contentRange string
}

// Range Reconstructs Range header and returns it
func (r *Range) Range() string {
        if r.Limit > 0 {
                return fmt.Sprintf("bytes=%d-%d", r.Start, r.Start+r.Limit-1)
        } else {
                return fmt.Sprintf("bytes=%d-", r.Start)
        }
}

// AcceptLength Tries to accept given range
// returns newContentLength if range was satisfied, otherwise returns given contentLength
func (r *Range) AcceptLength(contentLength uint64) (newContentLength uint64) {
        newContentLength = contentLength
        if r.Limit == 0 {
                r.Limit = newContentLength - r.Start
        }
        if contentLength < r.Start {
                return
        }
        if r.Limit > contentLength-r.Start {
                return
        }
        r.contentRange = fmt.Sprintf("bytes %d-%d/%d", r.Start, r.Start+r.Limit-1, contentLength)
        newContentLength = r.Limit
        return
}

func (r *Range) SetContentRange(cr string) {
        r.contentRange = cr
}

// Returns accepted Content-Range header. If range wasn't accepted empty string is returned
func (r *Range) ContentRange() string {
        return r.contentRange
}

var rexp *regexp.Regexp = regexp.MustCompile(`^bytes=([0-9]+)-([0-9]*)$`)

// Parses HTTP Range header and returns struct on success
// only bytes=start-finish supported
func ParseRange(rng string) *Range {
        if rng == "" {
                return nil
        }

        matches := rexp.FindAllStringSubmatch(rng, -1)
        if len(matches) != 1 || len(matches[0]) != 3 {
                return nil
        }
        if len(matches[0][0]) != len(rng) || len(matches[0][1]) == 0 {
                return nil
        }

        start, err := strconv.ParseUint(matches[0][1], 10, 64)
        if err != nil {
                return nil
        }

        if len(matches[0][2]) == 0 {
                return &Range{Start: start, Limit: 0}
        }

        finish, err := strconv.ParseUint(matches[0][2], 10, 64)
        if err != nil {
                return nil
        }
        if finish < start || finish+1 < finish {
                return nil
        }

        return &Range{Start: start, Limit: finish - start + 1}
}

// Storage is the interface for storage operation
type Storage interface {
        // Get retrieves a file from storage
        Get(ctx context.Context, token string, filename string, rng *Range) (reader io.ReadCloser, contentLength uint64, err error)
        // Head retrieves content length of a file from storage
        Head(ctx context.Context, token string, filename string) (contentLength uint64, err error)
        // Put saves a file on storage
        Put(ctx context.Context, token string, filename string, reader io.Reader, contentType string, contentLength uint64) error
        // Delete removes a file from storage
        Delete(ctx context.Context, token string, filename string) error
        // IsNotExist indicates if a file doesn't exist on storage
        IsNotExist(err error) bool
        // Purge cleans up the storage
        Purge(ctx context.Context, days time.Duration) error
        // Whether storage supports Get with Range header
        IsRangeSupported() bool
        // Type returns the storage type
        Type() string
}

func CloseCheck(c io.Closer) {
        if c == nil {
                return
        }

        if err := c.Close(); err != nil {
                fmt.Println("Received close error:", err)
        }
}

  build: package storage

import (
        "context"
        "encoding/json"
        "fmt"
        "io"
        "log"
        "net/http"
        "os"
        "path/filepath"
        "strings"
        "time"

        "golang.org/x/oauth2"
        "golang.org/x/oauth2/google"
        "google.golang.org/api/drive/v3"
        "google.golang.org/api/googleapi"
        "google.golang.org/api/option"
)

// GDrive is a storage backed by GDrive
type GDrive struct {
        service         *drive.Service
        rootID          string
        basedir         string
        localConfigPath string
        chunkSize       int
        logger          *log.Logger
}

const gDriveRootConfigFile = "root_id.conf"
const gDriveTokenJSONFile = "token.json"
const gDriveDirectoryMimeType = "application/vnd.google-apps.folder"

// NewGDriveStorage is the factory for GDrive
func NewGDriveStorage(ctx context.Context, clientJSONFilepath string, localConfigPath string, basedir string, chunkSize int, logger *log.Logger) (*GDrive, error) {

        b, err := os.ReadFile(clientJSONFilepath)
        if err != nil {
                return nil, err
        }

        // If modifying these scopes, delete your previously saved client_secret.json.
        config, err := google.ConfigFromJSON(b, drive.DriveScope, drive.DriveMetadataScope)
        if err != nil {
                return nil, err
        }

        httpClient := getGDriveClient(ctx, config, localConfigPath, logger)

        srv, err := drive.NewService(ctx, option.WithHTTPClient(httpClient))
        if err != nil {
                return nil, err
        }

        storage := &GDrive{service: srv, basedir: basedir, rootID: "", localConfigPath: localConfigPath, chunkSize: chunkSize, logger: logger}
        err = storage.setupRoot()
        if err != nil {
                return nil, err
        }

        return storage, nil
}

func (s *GDrive) setupRoot() error {
        rootFileConfig := filepath.Join(s.localConfigPath, gDriveRootConfigFile)

        rootID, err := os.ReadFile(rootFileConfig)
        if err != nil && !os.IsNotExist(err) {
                return err
        }

        if string(rootID) != "" {
                s.rootID = string(rootID)
                return nil
        }

        dir := &drive.File{
                Name:     s.basedir,
                MimeType: gDriveDirectoryMimeType,
        }

        di, err := s.service.Files.Create(dir).Fields("id").Do()
        if err != nil {
                return err
        }

        s.rootID = di.Id
        err = os.WriteFile(rootFileConfig, []byte(s.rootID), os.FileMode(0600))
        if err != nil {
                return err
        }

        return nil
}

func (s *GDrive) hasChecksum(f *drive.File) bool {
        return f.Md5Checksum != ""
}

func (s *GDrive) list(nextPageToken string, q string) (*drive.FileList, error) {
        return s.service.Files.List().Fields("nextPageToken, files(id, name, mimeType)").Q(q).PageToken(nextPageToken).Do()
}

func (s *GDrive) findID(filename string, token string) (string, error) {
        filename = strings.Replace(filename, `'`, `\'`, -1)
        filename = strings.Replace(filename, `"`, `\"`, -1)

        fileID, tokenID, nextPageToken := "", "", ""

        q := fmt.Sprintf("'%s' in parents and name='%s' and mimeType='%s' and trashed=false", s.rootID, token, gDriveDirectoryMimeType)
        l, err := s.list(nextPageToken, q)
        if err != nil {
                return "", err
        }

        for 0 < len(l.Files) {
                for _, fi := range l.Files {
                        tokenID = fi.Id
                        break
                }

                if l.NextPageToken == "" {
                        break
                }

                l, err = s.list(l.NextPageToken, q)
                if err != nil {
                        return "", err
                }
        }

        if filename == "" {
                return tokenID, nil
        } else if tokenID == "" {
                return "", fmt.Errorf("cannot find file %s/%s", token, filename)
        }

        q = fmt.Sprintf("'%s' in parents and name='%s' and mimeType!='%s' and trashed=false", tokenID, filename, gDriveDirectoryMimeType)
        l, err = s.list(nextPageToken, q)
        if err != nil {
                return "", err
        }

        for 0 < len(l.Files) {
                for _, fi := range l.Files {

                        fileID = fi.Id
                        break
                }

                if l.NextPageToken == "" {
                        break
                }

                l, err = s.list(l.NextPageToken, q)
                if err != nil {
                        return "", err
                }
        }

        if fileID == "" {
                return "", fmt.Errorf("cannot find file %s/%s", token, filename)
        }

        return fileID, nil
}

// Type returns the storage type
func (s *GDrive) Type() string {
        return "gdrive"
}

// Head retrieves content length of a file from storage
func (s *GDrive) Head(ctx context.Context, token string, filename string) (contentLength uint64, err error) {
        var fileID string
        fileID, err = s.findID(filename, token)
        if err != nil {
                return
        }

        var fi *drive.File
        if fi, err = s.service.Files.Get(fileID).Context(ctx).Fields("size").Do(); err != nil {
                return
        }

        contentLength = uint64(fi.Size)

        return
}

// Get retrieves a file from storage
func (s *GDrive) Get(ctx context.Context, token string, filename string, rng *Range) (reader io.ReadCloser, contentLength uint64, err error) {
        var fileID string
        fileID, err = s.findID(filename, token)
        if err != nil {
                return
        }

        var fi *drive.File
        fi, err = s.service.Files.Get(fileID).Fields("size", "md5Checksum").Do()
        if err != nil {
                return
        }
        if !s.hasChecksum(fi) {
                err = fmt.Errorf("cannot find file %s/%s", token, filename)
                return
        }

        contentLength = uint64(fi.Size)

        fileGetCall := s.service.Files.Get(fileID)
        if rng != nil {
                header := fileGetCall.Header()
                header.Set("Range", rng.Range())
        }

        var res *http.Response
        res, err = fileGetCall.Context(ctx).Download()
        if err != nil {
                return
        }

        if rng != nil {
                reader = res.Body
                rng.AcceptLength(contentLength)
                return
        }

        reader = res.Body

        return
}

// Delete removes a file from storage
func (s *GDrive) Delete(ctx context.Context, token string, filename string) (err error) {
        metadata, _ := s.findID(fmt.Sprintf("%s.metadata", filename), token)
        _ = s.service.Files.Delete(metadata).Do()

        var fileID string
        fileID, err = s.findID(filename, token)
        if err != nil {
                return
        }

        err = s.service.Files.Delete(fileID).Context(ctx).Do()
        return
}

// Purge cleans up the storage
func (s *GDrive) Purge(ctx context.Context, days time.Duration) (err error) {
        nextPageToken := ""

        expirationDate := time.Now().Add(-1 * days).Format(time.RFC3339)
        q := fmt.Sprintf("'%s' in parents and modifiedTime < '%s' and mimeType!='%s' and trashed=false", s.rootID, expirationDate, gDriveDirectoryMimeType)
        l, err := s.list(nextPageToken, q)
        if err != nil {
                return err
        }

        for 0 < len(l.Files) {
                for _, fi := range l.Files {
                        err = s.service.Files.Delete(fi.Id).Context(ctx).Do()
                        if err != nil {
                                return
                        }
                }

                if l.NextPageToken == "" {
                        break
                }

                l, err = s.list(l.NextPageToken, q)
                if err != nil {
                        return
                }
        }

        return
}

// IsNotExist indicates if a file doesn't exist on storage
func (s *GDrive) IsNotExist(err error) bool {
        if err == nil {
                return false
        }

        if e, ok := err.(*googleapi.Error); ok {
                return e.Code == http.StatusNotFound
        }

        return false
}

// Put saves a file on storage
func (s *GDrive) Put(ctx context.Context, token string, filename string, reader io.Reader, contentType string, contentLength uint64) error {
        dirID, err := s.findID("", token)
        if err != nil {
                return err
        }

        if dirID == "" {
                dir := &drive.File{
                        Name:     token,
                        Parents:  []string{s.rootID},
                        MimeType: gDriveDirectoryMimeType,
                }

                di, err := s.service.Files.Create(dir).Fields("id").Do()
                if err != nil {
                        return err
                }

                dirID = di.Id
        }

        // Instantiate empty drive file
        dst := &drive.File{
                Name:     filename,
                Parents:  []string{dirID},
                MimeType: contentType,
        }

        _, err = s.service.Files.Create(dst).Context(ctx).Media(reader, googleapi.ChunkSize(s.chunkSize)).Do()

        if err != nil {
                return err
        }

        return nil
}

func (s *GDrive) IsRangeSupported() bool { return true }

// Retrieve a token, saves the token, then returns the generated client.
func getGDriveClient(ctx context.Context, config *oauth2.Config, localConfigPath string, logger *log.Logger) *http.Client {
        tokenFile := filepath.Join(localConfigPath, gDriveTokenJSONFile)
        tok, err := gDriveTokenFromFile(tokenFile)
        if err != nil {
                tok = getGDriveTokenFromWeb(ctx, config, logger)
                saveGDriveToken(tokenFile, tok, logger)
        }

        return config.Client(ctx, tok)
}

// Request a token from the web, then returns the retrieved token.
func getGDriveTokenFromWeb(ctx context.Context, config *oauth2.Config, logger *log.Logger) *oauth2.Token {
        authURL := config.AuthCodeURL("state-token", oauth2.AccessTypeOffline)
        fmt.Printf("Go to the following link in your browser then type the "+
                "authorization code: \n%v\n", authURL)

        var authCode string
        if _, err := fmt.Scan(&authCode); err != nil {
                logger.Fatalf("Unable to read authorization code %v", err)
        }

        tok, err := config.Exchange(ctx, authCode)
        if err != nil {
                logger.Fatalf("Unable to retrieve token from web %v", err)
        }
        return tok
}

// Retrieves a token from a local file.
func gDriveTokenFromFile(file string) (*oauth2.Token, error) {
        f, err := os.Open(file)
        defer CloseCheck(f)
        if err != nil {
                return nil, err
        }
        tok := &oauth2.Token{}
        err = json.NewDecoder(f).Decode(tok)
        return tok, err
}

// Saves a token to a file path.
func saveGDriveToken(path string, token *oauth2.Token, logger *log.Logger) {
        logger.Printf("Saving credential file to: %s\n", path)
        f, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0600)
        defer CloseCheck(f)
        if err != nil {
                logger.Fatalf("Unable to cache oauth token: %v", err)
        }

        err = json.NewEncoder(f).Encode(token)
        if err != nil {
                logger.Fatalf("Unable to encode oauth token: %v", err)
        }
}
    runs-on: package storage

import (
        "context"
        "fmt"
        "io"
        "log"
        "os"
        "path/filepath"
        "time"
)

// LocalStorage is a local storage
type LocalStorage struct {
        Storage
        basedir string
        logger  *log.Logger
}

// NewLocalStorage is the factory for LocalStorage
func NewLocalStorage(basedir string, logger *log.Logger) (*LocalStorage, error) {
        return &LocalStorage{basedir: basedir, logger: logger}, nil
}

// Type returns the storage type
func (s *LocalStorage) Type() string {
        return "local"
}

// Head retrieves content length of a file from storage
func (s *LocalStorage) Head(_ context.Context, token string, filename string) (contentLength uint64, err error) {
        path := filepath.Join(s.basedir, token, filename)

        var fi os.FileInfo
        if fi, err = os.Lstat(path); err != nil {
                return
        }

        contentLength = uint64(fi.Size())

        return
}

// Get retrieves a file from storage
func (s *LocalStorage) Get(_ context.Context, token string, filename string, rng *Range) (reader io.ReadCloser, contentLength uint64, err error) {
        path := filepath.Join(s.basedir, token, filename)

        var file *os.File

        // content type , content length
        if file, err = os.Open(path); err != nil {
                return
        }
        reader = file

        var fi os.FileInfo
        if fi, err = os.Lstat(path); err != nil {
                return
        }

        contentLength = uint64(fi.Size())
        if rng != nil {
                contentLength = rng.AcceptLength(contentLength)
                if _, err = file.Seek(int64(rng.Start), 0); err != nil {
                        return
                }
        }

        return
}

// Delete removes a file from storage
func (s *LocalStorage) Delete(_ context.Context, token string, filename string) (err error) {
        metadata := filepath.Join(s.basedir, token, fmt.Sprintf("%s.metadata", filename))
        _ = os.Remove(metadata)

        path := filepath.Join(s.basedir, token, filename)
        err = os.Remove(path)
        return
}

// Purge cleans up the storage
func (s *LocalStorage) Purge(_ context.Context, days time.Duration) (err error) {
        err = filepath.Walk(s.basedir,
                func(path string, info os.FileInfo, err error) error {
                        if err != nil {
                                return err
                        }
                        if info.IsDir() {
                                return nil
                        }

                        if info.ModTime().Before(time.Now().Add(-1 * days)) {
                                err = os.Remove(path)
                                return err
                        }

                        return nil
                })

        return
}

// IsNotExist indicates if a file doesn't exist on storage
func (s *LocalStorage) IsNotExist(err error) bool {
        if err == nil {
                return false
        }

        return os.IsNotExist(err)
}

// Put saves a file on storage
func (s *LocalStorage) Put(_ context.Context, token string, filename string, reader io.Reader, contentType string, contentLength uint64) error {
        var f io.WriteCloser
        var err error

        path := filepath.Join(s.basedir, token)

        if err = os.MkdirAll(path, 0700); err != nil && !os.IsExist(err) {
                return err
        }

        f, err = os.OpenFile(filepath.Join(path, filename), os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0600)
        defer CloseCheck(f)

        if err != nil {
                return err
        }

        if _, err = io.Copy(f, reader); err != nil {
                return err
        }

        return nil
}

func (s *LocalStorage) IsRangeSupported() bool { return true }
    steps: package storage

import (
        "context"
        "errors"
        "fmt"
        "io"
        "log"
        "time"

        "github.com/aws/aws-sdk-go-v2/aws"
        "github.com/aws/aws-sdk-go-v2/config"
        "github.com/aws/aws-sdk-go-v2/credentials"
        "github.com/aws/aws-sdk-go-v2/feature/s3/manager"
        "github.com/aws/aws-sdk-go-v2/service/s3"
        "github.com/aws/aws-sdk-go-v2/service/s3/types"
)

// S3Storage is a storage backed by AWS S3
type S3Storage struct {
        Storage
        bucket      string
        s3          *s3.Client
        logger      *log.Logger
        purgeDays   time.Duration
        noMultipart bool
}

// NewS3Storage is the factory for S3Storage
func NewS3Storage(ctx context.Context, accessKey, secretKey, bucketName string, purgeDays int, region, endpoint string, disableMultipart bool, forcePathStyle bool, logger *log.Logger) (*S3Storage, error) {
        cfg, err := getAwsConfig(ctx, accessKey, secretKey)
        if err != nil {
                return nil, err
        }

        client := s3.NewFromConfig(cfg, func(o *s3.Options) {
                o.Region = region
                o.UsePathStyle = forcePathStyle
                if len(endpoint) > 0 {
                        o.EndpointResolver = s3.EndpointResolverFromURL(endpoint)
                }
        })

        return &S3Storage{
                bucket:      bucketName,
                s3:          client,
                logger:      logger,
                noMultipart: disableMultipart,
                purgeDays:   time.Duration(purgeDays*24) * time.Hour,
        }, nil
}

// Type returns the storage type
func (s *S3Storage) Type() string {
        return "s3"
}

// Head retrieves content length of a file from storage
func (s *S3Storage) Head(ctx context.Context, token string, filename string) (contentLength uint64, err error) {
        key := fmt.Sprintf("%s/%s", token, filename)

        headRequest := &s3.HeadObjectInput{
                Bucket: aws.String(s.bucket),
                Key:    aws.String(key),
        }

        // content type , content length
        response, err := s.s3.HeadObject(ctx, headRequest)
        if err != nil {
                return
        }

        contentLength = uint64(response.ContentLength)

        return
}

// Purge cleans up the storage
func (s *S3Storage) Purge(context.Context, time.Duration) (err error) {
        // NOOP expiration is set at upload time
        return nil
}

// IsNotExist indicates if a file doesn't exist on storage
func (s *S3Storage) IsNotExist(err error) bool {
        if err == nil {
                return false
        }

        var nkerr *types.NoSuchKey
        return errors.As(err, &nkerr)
}

// Get retrieves a file from storage
func (s *S3Storage) Get(ctx context.Context, token string, filename string, rng *Range) (reader io.ReadCloser, contentLength uint64, err error) {
        key := fmt.Sprintf("%s/%s", token, filename)

        getRequest := &s3.GetObjectInput{
                Bucket: aws.String(s.bucket),
                Key:    aws.String(key),
        }

        if rng != nil {
                getRequest.Range = aws.String(rng.Range())
        }

        response, err := s.s3.GetObject(ctx, getRequest)
        if err != nil {
                return
        }

        contentLength = uint64(response.ContentLength)
        if rng != nil && response.ContentRange != nil {
                rng.SetContentRange(*response.ContentRange)
        }

        reader = response.Body
        return
}

// Delete removes a file from storage
func (s *S3Storage) Delete(ctx context.Context, token string, filename string) (err error) {
        metadata := fmt.Sprintf("%s/%s.metadata", token, filename)
        deleteRequest := &s3.DeleteObjectInput{
                Bucket: aws.String(s.bucket),
                Key:    aws.String(metadata),
        }

        _, err = s.s3.DeleteObject(ctx, deleteRequest)
        if err != nil {
                return
        }

        key := fmt.Sprintf("%s/%s", token, filename)
        deleteRequest = &s3.DeleteObjectInput{
                Bucket: aws.String(s.bucket),
                Key:    aws.String(key),
        }

        _, err = s.s3.DeleteObject(ctx, deleteRequest)

        return
}

// Put saves a file on storage
func (s *S3Storage) Put(ctx context.Context, token string, filename string, reader io.Reader, contentType string, _ uint64) (err error) {
        key := fmt.Sprintf("%s/%s", token, filename)

        s.logger.Printf("Uploading file %s to S3 Bucket", filename)
        var concurrency int
        if !s.noMultipart {
                concurrency = 20
        } else {
                concurrency = 1
        }

        // Create an uploader with the session and custom options
        uploader := manager.NewUploader(s.s3, func(u *manager.Uploader) {
                u.Concurrency = concurrency // default is 5
                u.LeavePartsOnError = false
        })

        var expire *time.Time
        if s.purgeDays.Hours() > 0 {
                expire = aws.Time(time.Now().Add(s.purgeDays))
        }

        _, err = uploader.Upload(ctx, &s3.PutObjectInput{
                Bucket:      aws.String(s.bucket),
                Key:         aws.String(key),
                Body:        reader,
                Expires:     expire,
                ContentType: aws.String(contentType),
        })

        return
}

func (s *S3Storage) IsRangeSupported() bool { return true }

func getAwsConfig(ctx context.Context, accessKey, secretKey string) (aws.Config, error) {
        return config.LoadDefaultConfig(ctx,
                config.WithCredentialsProvider(credentials.StaticCredentialsProvider{
                        Value: aws.Credentials{
                                AccessKeyID:     accessKey,
                                SecretAccessKey: secretKey,
                                SessionToken:    "",
                        },
                }),
        )
}
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v4
      with: package storage

import (
        "context"
        "errors"
        "io"
        "log"
        "time"

        "storj.io/common/fpath"
        "storj.io/common/storj"
        "storj.io/uplink"
)

// StorjStorage is a storage backed by Storj
type StorjStorage struct {
        Storage
        project   *uplink.Project
        bucket    *uplink.Bucket
        purgeDays time.Duration
        logger    *log.Logger
}

// NewStorjStorage is the factory for StorjStorage
func NewStorjStorage(ctx context.Context, access, bucket string, purgeDays int, logger *log.Logger) (*StorjStorage, error) {
        var instance StorjStorage
        var err error

        ctx = fpath.WithTempData(ctx, "", true)

        uplConf := &uplink.Config{
                UserAgent: "transfer-sh",
        }

        parsedAccess, err := uplink.ParseAccess(access)
        if err != nil {
                return nil, err
        }

        instance.project, err = uplConf.OpenProject(ctx, parsedAccess)
        if err != nil {
                return nil, err
        }

        instance.bucket, err = instance.project.EnsureBucket(ctx, bucket)
        if err != nil {
                //Ignoring the error to return the one that occurred first, but try to clean up.
                _ = instance.project.Close()
                return nil, err
        }

        instance.purgeDays = time.Duration(purgeDays*24) * time.Hour

        instance.logger = logger

        return &instance, nil
}

// Type returns the storage type
func (s *StorjStorage) Type() string {
        return "storj"
}

// Head retrieves content length of a file from storage
func (s *StorjStorage) Head(ctx context.Context, token string, filename string) (contentLength uint64, err error) {
        key := storj.JoinPaths(token, filename)

        obj, err := s.project.StatObject(fpath.WithTempData(ctx, "", true), s.bucket.Name, key)
        if err != nil {
                return 0, err
        }

        contentLength = uint64(obj.System.ContentLength)

        return
}

// Get retrieves a file from storage
func (s *StorjStorage) Get(ctx context.Context, token string, filename string, rng *Range) (reader io.ReadCloser, contentLength uint64, err error) {
        key := storj.JoinPaths(token, filename)

        s.logger.Printf("Getting file %s from Storj Bucket", filename)

        var options *uplink.DownloadOptions
        if rng != nil {
                options = new(uplink.DownloadOptions)
                options.Offset = int64(rng.Start)
                if rng.Limit > 0 {
                        options.Length = int64(rng.Limit)
                } else {
                        options.Length = -1
                }
        }

        download, err := s.project.DownloadObject(fpath.WithTempData(ctx, "", true), s.bucket.Name, key, options)
        if err != nil {
                return nil, 0, err
        }

        contentLength = uint64(download.Info().System.ContentLength)
        if rng != nil {
                contentLength = rng.AcceptLength(contentLength)
        }

        reader = download
        return
}

// Delete removes a file from storage
func (s *StorjStorage) Delete(ctx context.Context, token string, filename string) (err error) {
        key := storj.JoinPaths(token, filename)

        s.logger.Printf("Deleting file %s from Storj Bucket", filename)

        _, err = s.project.DeleteObject(fpath.WithTempData(ctx, "", true), s.bucket.Name, key)

        return
}

// Purge cleans up the storage
func (s *StorjStorage) Purge(context.Context, time.Duration) (err error) {
        // NOOP expiration is set at upload time
        return nil
}

// Put saves a file on storage
func (s *StorjStorage) Put(ctx context.Context, token string, filename string, reader io.Reader, contentType string, contentLength uint64) (err error) {
        key := storj.JoinPaths(token, filename)

        s.logger.Printf("Uploading file %s to Storj Bucket", filename)

        var uploadOptions *uplink.UploadOptions
        if s.purgeDays.Hours() > 0 {
                uploadOptions = &uplink.UploadOptions{Expires: time.Now().Add(s.purgeDays)}
        }

        writer, err := s.project.UploadObject(fpath.WithTempData(ctx, "", true), s.bucket.Name, key, uploadOptions)
        if err != nil {
                return err
        }

        n, err := io.Copy(writer, reader)
        if err != nil || uint64(n) != contentLength {
                //Ignoring the error to return the one that occurred first, but try to clean up.
                _ = writer.Abort()
                return err
        }
        err = writer.SetCustomMetadata(ctx, uplink.CustomMetadata{"content-type": contentType})
        if err != nil {
                //Ignoring the error to return the one that occurred first, but try to clean up.
                _ = writer.Abort()
                return err
        }

        err = writer.Commit()
        return err
}

func (s *StorjStorage) IsRangeSupported() bool { return true }

// IsNotExist indicates if a file doesn't exist on storage
func (s *StorjStorage) IsNotExist(err error) bool {
        return errors.Is(err, uplink.ErrObjectNotFound)
}
        go-version: '1.20'

    - name: Build
      run: go build -v ./...

    - name: Test
      run: go test -v ./...
